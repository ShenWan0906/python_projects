import pandas as pd
import random
from sqlalchemy import text
from dbhelp import engine
from tqdm import tqdm
import time
import logging
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    print("ğŸ”„ å¼€å§‹å¤„ç†è®¾å¤‡åœ°åŒºæ•°æ®åˆ†é…...")

    # 1ï¸âƒ£ è¯»å– alabo_region å…¨éƒ¨åœ°åŒºæ•°æ®
    print("ğŸ“Š è¯»å–åœ°åŒºæ•°æ®...")
    sql_alabo_region = text("SELECT region_id, parent_id, name_en, level FROM alabo_region")
    df_region = pd.read_sql(sql_alabo_region, con=engine)

    print(f"alabo_region æ€»è®°å½•æ•°: {len(df_region)}")

    # 2ï¸âƒ£ åˆ†åˆ«ç­›é€‰å‡º çœã€å¸‚ã€åŒº
    df_province = df_region[df_region['level'] == "1"]
    df_city = df_region[df_region['level'] == "2"]
    df_area = df_region[df_region['level'] == "3"]

    print(f"çœä»½æ•°: {len(df_province)}")
    print(f"åŸå¸‚æ•°: {len(df_city)}")
    print(f"åŒºå¿æ•°: {len(df_area)}")

    # 3ï¸âƒ£ æ„é€ å®Œæ•´çš„ çœ-å¸‚-åŒº ç»„åˆ
    print("ğŸ”— æ„å»ºåœ°åŒºç»„åˆå…³ç³»...")
    region_list = []

    for _, prov in tqdm(df_province.iterrows(), total=len(df_province), desc="å¤„ç†çœä»½"):
        prov_id = prov['region_id']
        prov_name = prov['name_en']

        cities = df_city[df_city['parent_id'] == prov_id]
        for _, city in cities.iterrows():
            city_id = city['region_id']
            city_name = city['name_en']

            areas = df_area[df_area['parent_id'] == city_id]
            for _, area in areas.iterrows():
                area_id = area['region_id']
                area_name = area['name_en']

                region_list.append({
                    'province_id': prov_id,
                    'prov_name': prov_name,
                    'city_id': city_id,
                    'city_name': city_name,
                    'region_id': area_id,
                    'region_name': area_name,
                    'address': f'{prov_name} / {city_name} / {area_name}'
                })

    print(f'ğŸ“Œ å¯ç”¨åœ°åŒºç»„åˆæ•°: {len(region_list)}')

    # 4ï¸âƒ£ è¯»å– dev_device_instance éœ€è¦èµ‹å€¼çš„æ•°æ®
    print("ğŸ“‹ è¯»å–è®¾å¤‡æ•°æ®...")
    sql_dev_device_instance = text("SELECT id FROM dev_device_instance")
    df_devices = pd.read_sql(sql_dev_device_instance, con=engine)

    print(f"ğŸ“Œ éœ€è¦èµ‹å€¼çš„è®¾å¤‡æ•°: {len(df_devices)}")

    # 5ï¸âƒ£ ä¸ºæ¯æ¡è®¾å¤‡æ•°æ®åˆ†é…éšæœºåœ°å€
    print("ğŸ¯ åˆ†é…éšæœºåœ°åŒº...")
    assigned_data = []

    for _, row in tqdm(df_devices.iterrows(), total=df_devices.shape[0], desc="åˆ†é…åœ°åŒº"):
        random_region = random.choice(region_list)

        assigned_data.append({
            'id': row['id'],
            'province_id': random_region['province_id'],
            'province_name': random_region['prov_name'],
            'city_id': random_region['city_id'],
            'city_name': random_region['city_name'],
            'region_id': random_region['region_id'],
            'region_name': random_region['region_name'],
            'address': random_region['address']
        })

    df_assigned = pd.DataFrame(assigned_data)

    # 6ï¸âƒ£ ä½¿ç”¨ä¿®å¤åçš„å®‰å…¨æ›´æ–°æ–¹æ³•
    print("ğŸš€ å¼€å§‹æå®‰å…¨æ›´æ–°æ•°æ®åº“...")
    success = ultra_safe_single_update_fixed(df_assigned)

    if success:
        print("âœ… è®¾å¤‡æ•°æ®åœ°åŒºèµ‹å€¼å®Œæˆï¼")
    else:
        print("âŒ æ›´æ–°è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")


def ultra_safe_single_update_fixed(df_assigned):
    """
    ä¿®å¤åçš„å•æ¡è®°å½•æ›´æ–°æ–¹æ³•ï¼Œè§£å†³SQLAlchemy 2.0äº‹åŠ¡é—®é¢˜
    """
    total_records = len(df_assigned)

    print(f"ğŸ”„ å¼€å§‹å•æ¡è®°å½•æ›´æ–°ï¼Œå…± {total_records} æ¡è®°å½•")

    success_count = 0
    fail_count = 0

    # åˆ›å»ºè¿›åº¦æ¡
    pbar = tqdm(total=total_records, desc="æ›´æ–°è¿›åº¦")

    for index, row in df_assigned.iterrows():
        # å°è¯•æ›´æ–°å½“å‰è®°å½•ï¼Œæœ€å¤šé‡è¯•5æ¬¡
        record_success = False
        retry_count = 0
        max_retries = 5

        while not record_success and retry_count < max_retries:
            try:
                # ä¿®å¤ï¼šä½¿ç”¨ engine.begin() è€Œä¸æ˜¯ engine.connect() + conn.begin()
                with engine.begin() as conn:
                    # è®¾ç½®å¾ˆçŸ­çš„é”ç­‰å¾…æ—¶é—´
                    conn.execute(text("SET innodb_lock_wait_timeout = 10"))

                    update_sql = text("""
                        UPDATE dev_device_instance
                        SET province_id = :province_id,
                            province_name = :province_name,
                            city_id = :city_id,
                            city_name = :city_name,
                            region_id = :region_id,
                            region_name = :region_name,    
                            address = :address
                        WHERE id = :id
                    """)
                    result = conn.execute(update_sql, {
                        'province_id': row['province_id'],
                        'province_name': row['province_name'],
                        'city_id': row['city_id'],
                        'city_name': row['city_name'],
                        'region_id': row['region_id'],
                        'region_name': row['region_name'],
                        'address': row['address'],
                        'id': row['id']
                    })

                    # æ£€æŸ¥æ˜¯å¦çœŸçš„æ›´æ–°äº†è®°å½•
                    if result.rowcount == 0:
                        logger.warning(f"è®°å½• {row['id']} æœªæ‰¾åˆ°ï¼Œå¯èƒ½å·²è¢«åˆ é™¤")

                record_success = True
                success_count += 1

            except Exception as e:
                retry_count += 1
                error_msg = str(e)

                if "Lock wait timeout" in error_msg or "1205" in error_msg:
                    # é”è¶…æ—¶ï¼Œç­‰å¾…åé‡è¯•
                    wait_time = 5 * retry_count
                    logger.warning(f"è®°å½• {row['id']} æ›´æ–°é”è¶…æ—¶ï¼Œç¬¬ {retry_count} æ¬¡é‡è¯•ï¼Œç­‰å¾… {wait_time} ç§’")
                    time.sleep(wait_time)
                else:
                    # å…¶ä»–é”™è¯¯
                    wait_time = 2 ** retry_count
                    logger.warning(f"è®°å½• {row['id']} æ›´æ–°å¤±è´¥ï¼Œç¬¬ {retry_count} æ¬¡é‡è¯•ï¼Œé”™è¯¯: {error_msg}")
                    time.sleep(wait_time)

                if retry_count >= max_retries:
                    logger.error(f"è®°å½• {row['id']} æ›´æ–°å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                    fail_count += 1
                    save_failed_record(row, index)

        # æ›´æ–°è¿›åº¦æ¡
        pbar.update(1)
        pbar.set_postfix(æˆåŠŸ=f"{success_count}", å¤±è´¥=f"{fail_count}")

        # æ¯å¤„ç†ä¸€å®šæ•°é‡è®°å½•åæš‚åœ
        if success_count > 0 and success_count % 100 == 0:
            time.sleep(0.5)  # çŸ­æš‚æš‚åœ

        # æ¯å¤„ç†1000æ¡è®°å½•åæ˜¾ç¤ºä¸€æ¬¡çŠ¶æ€
        if success_count > 0 and success_count % 1000 == 0:
            logger.info(f"å·²å¤„ç† {success_count} æ¡è®°å½•ï¼Œå¤±è´¥ {fail_count} æ¡")

    pbar.close()

    print(f"ğŸ“Š æ›´æ–°å®Œæˆç»Ÿè®¡:")
    print(f"  æˆåŠŸè®°å½•: {success_count}")
    print(f"  å¤±è´¥è®°å½•: {fail_count}")
    print(f"  æˆåŠŸç‡: {success_count / total_records * 100:.2f}%")

    return fail_count == 0


def ultra_safe_batch_update(df_assigned):
    """
    å¤‡é€‰æ–¹æ¡ˆï¼šå°æ‰¹æ¬¡æ›´æ–°ï¼Œæ•ˆç‡æ›´é«˜
    """
    batch_size = 50  # å°æ‰¹æ¬¡å¤§å°
    total_batches = (len(df_assigned) + batch_size - 1) // batch_size

    print(f"ğŸ”„ å¼€å§‹å°æ‰¹æ¬¡æ›´æ–°ï¼Œå…± {total_batches} æ‰¹ï¼Œæ¯æ‰¹ {batch_size} æ¡è®°å½•")

    success_count = 0
    fail_count = 0

    pbar = tqdm(total=total_batches, desc="æ‰¹æ¬¡è¿›åº¦")

    for batch_num in range(total_batches):
        start_idx = batch_num * batch_size
        end_idx = min((batch_num + 1) * batch_size, len(df_assigned))
        batch_df = df_assigned.iloc[start_idx:end_idx]

        batch_success = False
        retry_count = 0
        max_retries = 3

        while not batch_success and retry_count < max_retries:
            try:
                with engine.begin() as conn:
                    conn.execute(text("SET innodb_lock_wait_timeout = 30"))

                    # ä½¿ç”¨ executemany æ‰¹é‡æ›´æ–°
                    update_sql = text("""
                        UPDATE dev_device_instance
                        SET province_id = :province_id,
                            province_name = :province_name,
                            city_id = :city_id,
                            city_name = :city_name,
                            region_id = :region_id,
                            region_name = :region_name,    
                            address = :address
                        WHERE id = :id
                    """)

                    params = []
                    for _, row in batch_df.iterrows():
                        params.append({
                            'province_id': row['province_id'],
                            'province_name': row['province_name'],
                            'city_id': row['city_id'],
                            'city_name': row['city_name'],
                            'region_id': row['region_id'],
                            'region_name': row['region_name'],
                            'address': row['address'],
                            'id': row['id']
                        })

                    conn.execute(update_sql, params)

                batch_success = True
                success_count += len(batch_df)

            except Exception as e:
                retry_count += 1
                error_msg = str(e)

                if "Lock wait timeout" in error_msg or "1205" in error_msg:
                    wait_time = 10 * retry_count
                    logger.warning(f"æ‰¹æ¬¡ {batch_num} æ›´æ–°é”è¶…æ—¶ï¼Œç¬¬ {retry_count} æ¬¡é‡è¯•ï¼Œç­‰å¾… {wait_time} ç§’")
                else:
                    wait_time = 5 * retry_count
                    logger.warning(f"æ‰¹æ¬¡ {batch_num} æ›´æ–°å¤±è´¥ï¼Œç¬¬ {retry_count} æ¬¡é‡è¯•ï¼Œé”™è¯¯: {error_msg}")

                time.sleep(wait_time)

                if retry_count >= max_retries:
                    logger.error(f"æ‰¹æ¬¡ {batch_num} æ›´æ–°å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                    fail_count += len(batch_df)
                    save_failed_batch(batch_df, batch_num)

        pbar.update(1)
        pbar.set_postfix(æˆåŠŸ=f"{success_count}", å¤±è´¥=f"{fail_count}")

        # æ‰¹æ¬¡é—´æš‚åœ
        if batch_success:
            time.sleep(0.1)

    pbar.close()

    print(f"ğŸ“Š æ›´æ–°å®Œæˆç»Ÿè®¡:")
    print(f"  æˆåŠŸè®°å½•: {success_count}")
    print(f"  å¤±è´¥è®°å½•: {fail_count}")
    print(f"  æˆåŠŸç‡: {success_count / len(df_assigned) * 100:.2f}%")

    return fail_count == 0


def save_failed_record(record, index):
    """ä¿å­˜å¤±è´¥è®°å½•åˆ°æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"failed_records_{timestamp}.csv"

    # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªå¤±è´¥è®°å½•ï¼Œåˆ›å»ºæ–‡ä»¶å¹¶å†™å…¥header
    if index == 0:
        pd.DataFrame([record]).to_csv(filename, index=False)
    else:
        # è¿½åŠ åˆ°ç°æœ‰æ–‡ä»¶
        pd.DataFrame([record]).to_csv(filename, mode='a', header=False, index=False)

    logger.info(f"å¤±è´¥è®°å½•å·²ä¿å­˜åˆ°: {filename}")


def save_failed_batch(batch_df, batch_num):
    """ä¿å­˜å¤±è´¥æ‰¹æ¬¡åˆ°æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"failed_batch_{batch_num}_{timestamp}.csv"
    batch_df.to_csv(filename, index=False)
    logger.info(f"å¤±è´¥æ‰¹æ¬¡å·²ä¿å­˜åˆ°: {filename}")


if __name__ == "__main__":
    start_time = time.time()

    try:
        main()
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logger.error(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸé”™è¯¯: {str(e)}")
        import traceback

        traceback.print_exc()
    finally:
        # è®¡ç®—æ€»è€—æ—¶
        end_time = time.time()
        total_time = end_time - start_time
        print(f"â° æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f} ç§’ ({total_time / 60:.2f} åˆ†é’Ÿ)")